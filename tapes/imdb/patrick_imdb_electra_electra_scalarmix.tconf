global {
    ducttape_output="/projects/tir2/users/pfernand/metaexpl-experiments/imdb-electra-electra-scalarmix"
    ducttape_experimental_submitters=true
    ducttape_experimental_imports=true
    repo="/home/pfernand/repos/meta-expl"

    task="imdb"
    task_params=""
    num_examples=(NumExamples: 1000 500 2000)
    metric="accuracy"

    # models and explainers parameters
    teacher_arch=electra
    student_arch=electra
    init_embed=false

    teacher_explainer=(
        TeacherExplainer:
            attention="attention_explainer"
            gradient_input="gradient_input_explainer"
            integrated_gradients="integrated_gradients_explainer"
    )
    student_explainer=attention_explainer

    teacher_normalizer_fn=(TeacherNormalizerFn: softmax topk_softmax)
    student_normalizer_fn=softmax
    teacher_init_fn=uniform
    teacher_aggregator_idx="\"mean\""
    student_aggregator_idx="\"mean\""
    teacher_aggregator_dim="\"row\""
    student_aggregator_dim="\"row\""
    parametrize_head_coeffs=true
    normalize_head_coeffs=(NormalizeHeadCoeffs: sparsemax="\"sparsemax\"" entmax="\"entmax\"" softmax=true none=false)
    teacher_layer_idx=(
        BestLayer: 
            false=null 
            true=11
    )
    student_layer_idx=(
        BestLayer:
            false=null
            true=11
    )
    aggregator_normalizer_fn=sparsemax

    # optimization paramaters
    student_optimizer=sgd
    num_epochs=""
    batch_size=32
    lr=5e-3
    kld_coeff=(KLDCoeff: 0 0.2 1 5)
    patience=10

    # (meta) optimization parameters
    metalearn=(
        MetaLearn:
            false=false
            true=true
    )
    meta_lr=(MetaLR: 1e-3 5e-3 1e-4 2e-4 5e-4 1e-5 5e-5)
    meta_interval=1
    meta_explicit=true
    meta_warmup=0
    num_resets=0

    seed=(Seed: 0 9 11 30 69)

    wandb_project="meta-expl-imdb-electra-electra-scalarmix"
    exclude="tir-0-[7,9,11,13,15,17,19,32,36],tir-1-11,tir-0-3,tir-1-7"

    submitter=slurm
}

plan TrainTeacher {
    reach TrainTeacher
}

plan TrainStudent {
    reach AverageResults via (NumExamples: 1000) * (KLDCoeff: 0)
    reach AverageResults via (NumExamples: 1000) * (KLDCoeff: 5) * (NormalizeHeadCoeffs: *)
}

plan MetaTrain {
    reach AverageResults via (NumExamples: 1000) * (KLDCoeff: 5) * (MetaLearn: true) * (MetaLR: 2e-4 5e-4 1e-3 5e-3) * (NormalizeHeadCoeffs: *)
}

plan GradientExplainers {
    reach AverageResults via (NumExamples: 1000) * (KLDCoeff: 0.2 1 5) * (TeacherExplainer: gradient_input integrated_gradients) 
}

plan PaperResults {
    reach AverageResults via (NumExamples: *) * (KLDCoeff: 0)
    reach AverageResults via (NumExamples: *) * (KLDCoeff: 5) * (TeacherExplainer: attention)
    reach AverageResults via (NumExamples: *) * (KLDCoeff: 5) * (TeacherExplainer: attention) * (BestLayer: true)
    reach AverageResults via (NumExamples: *) * (KLDCoeff: 0.2) * (TeacherExplainer: gradient_input integrated_gradients)
    reach AverageResults via (NumExamples: *) * (KLDCoeff: 5)   * (TeacherExplainer: attention) * (MetaLearn: true) * (MetaLR: 5e-4)
}
