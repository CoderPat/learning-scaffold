{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0343ff12",
   "metadata": {},
   "source": [
    "Install dependencies for computing metrics and plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7502e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install numpy scipy pandas seaborn matplotlib sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1d560",
   "metadata": {},
   "source": [
    "## Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from entmax_jax import sparsemax\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from functools import partial\n",
    "import json\n",
    "\n",
    "from smat.explainers import load_explainer\n",
    "from smat.models import load_model\n",
    "from smat.data.mlqe import dataloader\n",
    "\n",
    "from evaluate_explanations import evaluate_word_level, evaluate_sentence_level, aggregate_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cdb896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot utils\n",
    "def detokenize(pieces):\n",
    "    return ''.join(pieces).replace('▁', ' ').replace('</s>', ' </s>')\n",
    "\n",
    "def colorize_twoway(words, color_array, label='GOLD', max_width_shown=1800):\n",
    "    template_pos = '<span class=\"barcode\"; style=\"color: black; background-color: rgba(0, 255, 0, {}); display:inline-block; font-size:12px;\">&nbsp {} &nbsp</span>'\n",
    "    template_neg = '<span class=\"barcode\"; style=\"color: black; background-color: rgba(255, 0, 0, {}); display:inline-block; font-size:12px;\">&nbsp {} &nbsp</span>'\n",
    "    colored_string = ''\n",
    "    f = lambda w: w.replace('<', 'ᐸ').replace('>', 'ᐳ')\n",
    "    for word, color in zip(words, color_array / color_array.abs().max()):\n",
    "        if color > 0:\n",
    "            colored_string += template_pos.format(color, f(word))\n",
    "        else:\n",
    "            colored_string += template_neg.format(-color, f(word))\n",
    "    html_text = '<div style=\"100%\">{}:&nbsp;&nbsp; {}</div>'.format(label, colored_string)\n",
    "    display(HTML(html_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data utils\n",
    "def read_data(lp, split='dev'):\n",
    "    def tags_to_ints(line):\n",
    "        return list(map(int, line.strip().replace('OK', '0').replace('BAD', '1').split()))\n",
    "    data = {\n",
    "        'original': [line.strip() for line in open('data/{}/{}.src'.format(lp, split), 'r')],\n",
    "        'translation': [line.strip() for line in open('data/{}/{}.mt'.format(lp, split), 'r')],\n",
    "        'da': [float(line.strip()) for line in open('data/{}/{}.da'.format(lp, split), 'r')],\n",
    "        'src_tags': [tags_to_ints(line) for line in open('data/{}/{}.src-tags'.format(lp, split), 'r')],\n",
    "        'mt_tags': [tags_to_ints(line) for line in open('data/{}/{}.tgt-tags'.format(lp, split), 'r')]\n",
    "    }\n",
    "    z = np.array(data['da'])\n",
    "    data['z_mean'] = (z - z.mean()) / z.std()\n",
    "    data = [dict(zip(data.keys(), v)) for v in list(zip(*data.values()))]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77dafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "arch = 'xlm-roberta-base'\n",
    "arch_mtl = 'xlm-r'\n",
    "setup = 'no_teacher'  # \"no_teacher\", \"static_teacher\", \"learnable_teacher\"\n",
    "lp = 'ro-en'\n",
    "max_len = 256\n",
    "batch_size = 1\n",
    "seed = 1\n",
    "sep_token = \"</s>\" if 'xlm' in arch else \"[SEP]\"\n",
    "num_classes = 1\n",
    "teacher_dir = 'data/mlqe-xlmr-explainer/teacher_dir'\n",
    "explainer_dir = 'data/mlqe-xlmr-explainer/teacher_expl_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy inputs for model instantiation\n",
    "input_ids = jnp.ones((batch_size, max_len), jnp.int32)\n",
    "dummy_inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": jnp.ones_like(input_ids),\n",
    "    \"token_type_ids\": jnp.arange(jnp.atleast_2d(input_ids).shape[-1]),\n",
    "    \"position_ids\": jnp.ones_like(input_ids),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b908c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataloader = partial(dataloader, sep_token=sep_token)\n",
    "train_data = read_data(lp, \"train\")\n",
    "valid_data = read_data(lp, \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf471df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizerFast\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher, teacher_params, dummy_state = load_model(teacher_dir, batch_size, max_len)\n",
    "explainer, explainer_params = load_explainer(explainer_dir, dummy_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6452578",
   "metadata": {},
   "source": [
    "## Evaluating explanations by comparing explanations with word-level QE tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "def get_predictions_and_explanations(data):\n",
    "    all_outputs = []\n",
    "    all_explanations_src, all_explanations_mt = [], []\n",
    "    for x, y in dataloader(data, tokenizer, batch_size=batch_size, max_len=max_len, shuffle=False):\n",
    "        outputs = explainer.apply(explainer_params, **x)[0]\n",
    "        all_outputs.append(outputs)\n",
    "        \n",
    "        #todo:\n",
    "        best_layer_id = 1\n",
    "        best_head_id = 1\n",
    "        src_explanation = explainer.attention.coefficients[best_layer_id][best_head_id]\n",
    "        src_explanation = aggregate_pieces(torch.from_numpy(src_explanations)).numpy()\n",
    "        all_expanations_src.append(src_explanation)\n",
    "\n",
    "    return all_outputs, all_explanations_src, all_explanations_mt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pred, explanations_src_pred, explanations_mt_pred = get_predictions_and_explanations(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating predictions\n",
    "explanations_src_gold = [inp['mt_tags'] for inp in valid_data]\n",
    "explanations_mt_gold = [inp['mt_tags'] for inp in valid_data]\n",
    "scores_gold = [inp['z_mean'] for inp in valid_data]\n",
    "explanations_src_pred = []\n",
    "explanations_mt_pred = []\n",
    "scores_pred = []\n",
    "softmax = lambda x: np.exp(x) / np.exp(x).sum()\n",
    "for inp in valid_data:\n",
    "    explanations_src_pred.append(softmax(np.random.randn(len(inp['src_tags']))))\n",
    "    explanations_mt_pred.append(softmax(np.random.randn(len(inp['mt_tags']))))\n",
    "    scores_pred.append(np.random.randn(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_word_level(explanations_mt_gold, explanations_mt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_sentence_level(scores_gold, scores_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f2d1f",
   "metadata": {},
   "source": [
    "## Visualizing explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_explanation(tokens, tags, explanation):\n",
    "    import torch\n",
    "    colorize_twoway(tokens.split(), torch.tensor(tags), label='gold')\n",
    "    colorize_twoway(tokens.split(), torch.tensor(explanation), label='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SRC:')\n",
    "print('----')\n",
    "show_explanation(valid_data[0]['original'], explanations_src_gold[0], explanations_src_pred[0])\n",
    "\n",
    "print('MT:')\n",
    "print('----')\n",
    "show_explanation(valid_data[0]['translation'], explanations_mt_gold[0], explanations_mt_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850fd06",
   "metadata": {},
   "source": [
    "## Plotting the distribution of predictions and AUC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define options for seaborn\n",
    "custom_params = {\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "    'grid.color': '.85',\n",
    "    'grid.linestyle': ':'\n",
    "}\n",
    "_ = sns.set_theme(style='whitegrid', rc=custom_params),\n",
    "\n",
    "def plot_da_vs_expl_metric(metric_fn, das, e_golds, e_preds):\n",
    "    x = []\n",
    "    y = []\n",
    "    for da, gold, pred in zip(das, e_golds, e_preds):\n",
    "        if sum(gold) == 0 or sum(gold) == len(gold):\n",
    "            continue\n",
    "        y.append(metric_fn(gold, pred))\n",
    "        x.append(da)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    sns.regplot(x=x, y=y, ax=axs[0], color='tab:green')\n",
    "    axs[0].set_xlabel('da')\n",
    "    axs[0].set_ylabel(str(metric_fn).split()[1])\n",
    "    sns.histplot(x, bins=20, ax=axs[1], color='green')\n",
    "    axs[1].set_xlabel('da')\n",
    "    sns.histplot(y, bins=20, ax=axs[2], color='green')\n",
    "    axs[2].set_xlabel(str(metric_fn).split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted DA vs AUC\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "plot_da_vs_expl_metric(roc_auc_score, scores_pred, explanations_mt_gold, explanations_mt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(16, 11))\n",
    "fig.tight_layout(pad=3)\n",
    "for i in range(6):\n",
    "    ax = axs[i//3, i%3]\n",
    "    tokens = valid_data[i]['translation'].split()\n",
    "    n = len(tokens)\n",
    "    A = np.stack([softmax(np.random.randn(n)) for _ in range(n)])\n",
    "    df = pd.DataFrame(A, columns=tokens, index=tokens)\n",
    "    sns.heatmap(df, vmax=1, cmap=\"Greens\", square=True, cbar=False, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
