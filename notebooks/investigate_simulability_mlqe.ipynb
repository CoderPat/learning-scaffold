{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f173f6e",
   "metadata": {},
   "source": [
    "set cuda id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5442597",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from entmax_jax.activations import sparsemax, entmax15\n",
    "\n",
    "from meta_expl.explainers import load_explainer\n",
    "from meta_expl.models import load_model\n",
    "from meta_expl.data.mlqe import dataloader\n",
    "from meta_expl.utils import PRNGSequence, mse_loss, pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data utils\n",
    "def unroll(list_of_lists):\n",
    "    return [e for ell in list_of_lists for e in ell]\n",
    "\n",
    "def read_data(lp, split='dev'):\n",
    "    def tags_to_ints(line):\n",
    "        return list(map(int, line.strip().replace('OK', '0').replace('BAD', '1').split()))\n",
    "    data = {\n",
    "        'original': [line.strip() for line in open('data/mlqepe/{}/{}.src'.format(lp, split), 'r')],\n",
    "        'translation': [line.strip() for line in open('data/mlqepe/{}/{}.mt'.format(lp, split), 'r')],\n",
    "        'z_mean': [float(line.strip()) for line in open('data/mlqepe/{}/{}.da'.format(lp, split), 'r')],\n",
    "        'src_tags': [tags_to_ints(line) for line in open('data/mlqepe/{}/{}.src-tags'.format(lp, split), 'r')],\n",
    "        'mt_tags': [tags_to_ints(line) for line in open('data/mlqepe/{}/{}.tgt-tags'.format(lp, split), 'r')]\n",
    "    }\n",
    "    data = [dict(zip(data.keys(), v)) for v in list(zip(*data.values()))]\n",
    "    return data\n",
    "\n",
    "def read_data_all(lps, split='dev'):\n",
    "    data = {\n",
    "        'original': [],\n",
    "        'translation': [],\n",
    "        'z_mean': [],\n",
    "        'src_tags': [],\n",
    "        'mt_tags': [],\n",
    "    }\n",
    "    for lp in lps:\n",
    "        ell = read_data(lp, split)\n",
    "        for key in data.keys():\n",
    "            data[key].extend([d[key] for d in ell])\n",
    "    data = [dict(zip(data.keys(), v)) for v in list(zip(*data.values()))]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0051472",
   "metadata": {},
   "source": [
    "## Define args and load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77dafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "arch = 'xlm-roberta-base'\n",
    "setup = 'static_teacher'  # \"no_teacher\", \"static_teacher\", \"learnable_teacher\"\n",
    "\n",
    "seed = 9\n",
    "max_len = 256\n",
    "batch_size = 16\n",
    "\n",
    "sep_token = \"</s>\" if 'xlm' in arch else \"[SEP]\"\n",
    "num_classes = 1\n",
    "task_type = \"regression\"\n",
    "criterion = mse_loss\n",
    "dataloader = partial(dataloader, sep_token=sep_token)\n",
    "\n",
    "teacher_dir = 'data/mlqe-xlmr-models/teacher_dir'\n",
    "teacher_expl_dir = 'data/mlqe-xlmr-models/teacher_expl_dir'\n",
    "student_dir = 'data/mlqe-xlmr-models/student_dir'\n",
    "student_expl_dir = 'data/mlqe-xlmr-models/student_expl_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy inputs for model instantiation\n",
    "input_ids = jnp.ones((batch_size, max_len), jnp.int32)\n",
    "dummy_inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": jnp.ones_like(input_ids),\n",
    "    \"token_type_ids\": jnp.arange(jnp.atleast_2d(input_ids).shape[-1]),\n",
    "    \"position_ids\": jnp.ones_like(input_ids),\n",
    "}\n",
    "dummy_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ea756",
   "metadata": {},
   "source": [
    "### load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf471df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizerFast\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(arch)\n",
    "cls_id = tokenizer.cls_token_id\n",
    "sep_id = tokenizer.sep_token_id\n",
    "pad_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df6d85",
   "metadata": {},
   "source": [
    "### load models and explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ee21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher, teacher_params, dummy_state = load_model(teacher_dir, batch_size, max_len)\n",
    "teacher_expl, teacher_expl_params = load_explainer(teacher_expl_dir, dummy_inputs, state=dummy_state)\n",
    "student, student_params, dummy_state = load_model(student_dir, batch_size, max_len)\n",
    "student_expl, student_expl_params = load_explainer(student_expl_dir, dummy_inputs, state=dummy_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa76a40",
   "metadata": {},
   "source": [
    "### create a fixed teacher explainer using a specific layer & head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ace362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from meta_expl.explainers import create_explainer\n",
    "# keyseq = PRNGSequence(11)\n",
    "# teacher_explainer_params={\n",
    "#     'normalize_head_coeffs': 'sparsemax',\n",
    "#     'normalizer_fn': 'softmax',\n",
    "#     'aggregator_idx': 'mean',\n",
    "#     'aggregator_dim': 'row',\n",
    "#     'init_fn': 'uniform',\n",
    "#     'layer_idx': 9,  #9, None\n",
    "#     'head_idx': 5,  #5, None\n",
    "# }\n",
    "# explainer_type='attention_explainer'\n",
    "# teacher_explainer, teacher_explainer_params = create_explainer(next(keyseq), dummy_inputs, dummy_state, \n",
    "#                                      explainer_type, explainer_args=teacher_explainer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d09885",
   "metadata": {},
   "source": [
    "### look at the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsemax(teacher_expl_params['params']['head_coeffs']).reshape(12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a706e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = sparsemax(teacher_expl_params['params']['head_coeffs']).reshape(12, 12)\n",
    "for a, b in zip(*hc.nonzero()):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18549d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the layers with the highest coefficients\n",
    "layer_coeffs = hc.mean(-1).tolist()\n",
    "sorted(list(zip(list(range(1, len(layer_coeffs)+1)), layer_coeffs)), key=lambda k: k[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355517e0",
   "metadata": {},
   "source": [
    "## Evaluate simulability and student performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bca532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, return_outputs=False):\n",
    "    teacher_predict = None\n",
    "    all_outputs, all_y_sim, all_y = [], [], []\n",
    "    for i, (x, y) in enumerate(dataloader(data, tokenizer, batch_size=batch_size, max_len=max_len, shuffle=False)):\n",
    "        print('{} of {}'.format(i, len(data)//batch_size), end='\\r')\n",
    "        y_sim = teacher.apply(teacher_params, **x)[0]\n",
    "        outputs = student.apply(student_params, **x)[0]\n",
    "        all_outputs.append(outputs)\n",
    "        all_y_sim.append(y_sim)\n",
    "        all_y.append(y)\n",
    "    all_outputs = jnp.concatenate(all_outputs, axis=0)\n",
    "    all_y_sim = jnp.concatenate(all_y_sim, axis=0)\n",
    "    all_y = jnp.concatenate(all_y, axis=0)\n",
    "    student_score = pearson(all_outputs, all_y)\n",
    "    teacher_score = pearson(all_y_sim, all_y)\n",
    "    sim_score = pearson(all_outputs, all_y_sim)\n",
    "    if return_outputs:\n",
    "        return final_score, sim_score, (all_outputs, all_y_sim, all_y)\n",
    "    return student_score, teacher_score, sim_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4f9ad",
   "metadata": {},
   "source": [
    "### Evaluate for each LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "langpairs = [\"en-de\", \"en-zh\", \"et-en\", \"ne-en\", \"ro-en\", \"ru-en\"]\n",
    "split = 'dev'\n",
    "for lp in langpairs:\n",
    "    print(lp)\n",
    "    student_score, teacher_score, sim_score = evaluate(read_data(lp, split))\n",
    "    print('------------')\n",
    "    print('Pearson (teacher): {:.4f}'.format(teacher_score))\n",
    "    print('Pearson (student): {:.4f}'.format(student_score))\n",
    "    print('Pearson (simulability): {:.4f}'.format(sim_score))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall\")\n",
    "student_score, teacher_score, sim_score = evaluate(read_data_all(langpairs, split))\n",
    "print('------------')\n",
    "print('Pearson (teacher): {:.4f}'.format(teacher_score))\n",
    "print('Pearson (student): {:.4f}'.format(student_score))\n",
    "print('Pearson (simulability): {:.4f}'.format(sim_score))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1636ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "langpairs = [\"en-de\", \"en-zh\", \"et-en\", \"ne-en\", \"ro-en\", \"ru-en\"]\n",
    "split = 'test'\n",
    "for lp in langpairs:\n",
    "    print(lp)\n",
    "    student_score, teacher_score, sim_score = evaluate(read_data(lp, split))\n",
    "    print('------------')\n",
    "    print('Pearson (teacher): {:.4f}'.format(teacher_score))\n",
    "    print('Pearson (student): {:.4f}'.format(student_score))\n",
    "    print('Pearson (simulability): {:.4f}'.format(sim_score))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f30371",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall\")\n",
    "student_score, teacher_score, sim_score = evaluate(read_data_all(langpairs, split))\n",
    "print('------------')\n",
    "print('Pearson (teacher): {:.4f}'.format(teacher_score))\n",
    "print('Pearson (student): {:.4f}'.format(student_score))\n",
    "print('Pearson (simulability): {:.4f}'.format(sim_score))\n",
    "print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
