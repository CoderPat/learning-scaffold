{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a481e759",
   "metadata": {},
   "source": [
    "#### MLQE XLMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de965d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-scalarmix/TrainTeacher/Baseline.baseline/teacher_dir\"\n",
    "\n",
    "explainers_dirs = {\n",
    "    \"attention\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers/TrainStudent/KLDCoeff.5+Seed.9/teacher_expl_dir\",\n",
    "    \"attention_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-scalarmix/TrainStudent/KLDCoeff.5+MetaLR.2e-4+MetaLearn.true+Seed.11/teacher_expl_dir\",\n",
    "    #\"attention_entmax_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-scalarmix/TrainStudent/KLDCoeff.5+MetaLR.2e-4+MetaLearn.true++NormalizeHeadCoeffs.entmax+Seed.11/teacher_expl_dir\",\n",
    "    #\"attention_softmax_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-scalarmix/TrainStudent/KLDCoeff.5+MetaLR.2e-4+MetaLearn.true++NormalizeHeadCoeffs.true+Seed.11/teacher_expl_dir\",\n",
    "    \"gradient_input_explainer\": None\n",
    "}\n",
    "\n",
    "\n",
    "seed=0\n",
    "max_len=256\n",
    "batch_size=1\n",
    "task=\"mlqe\"\n",
    "tokenizer=\"xlm-r\"\n",
    "model_dims = [12, 12]\n",
    "modality=\"text\"\n",
    "from smat.utils import pearson\n",
    "metric=pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers/TrainTeacher/Baseline.baseline/teacher_dir\"\n",
    "\n",
    "explainers_dirs = {\n",
    "    \"attention\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers/TrainStudent/KLDCoeff.5+Seed.9/teacher_expl_dir\",\n",
    "    \"attention_softmax_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true+NormalizeHeadCoeffs.softmax/teacher_expl_dir\",\n",
    "    \"attention_sparsemax_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true/teacher_expl_dir\",\n",
    "    \"attention_entmax_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true+NormalizeHeadCoeffs.entmax/teacher_expl_dir\",\n",
    "}\n",
    "\n",
    "#explainers_dirs = {\n",
    "#    \"attention\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers/TrainStudent/KLDCoeff.5+Seed.9/teacher_expl_dir\",\n",
    "#    \"topk_attention\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+TeacherNormalizerFn.topk_softmax/teacher_expl_dir\",\n",
    "#    \"attention_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true/teacher_expl_dir\",\n",
    "#    \"topk_attention_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true+Seed.11+TeacherNormalizerFn.topk_softmax/teacher_expl_dir\",\n",
    "#}\n",
    "\n",
    "#explainers_dirs = {\n",
    "#    \"attention\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers/TrainStudent/KLDCoeff.5+Seed.9/teacher_expl_dir\",\n",
    "#    \"attention_trained_1k\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true+NumExamples.1050/teacher_expl_dir\",\n",
    "#    \"attention_trained_2k\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true+NumExamples.2100/teacher_expl_dir\",\n",
    "#    \"attention_trained_4k\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true/teacher_expl_dir\",\n",
    "#    \"attention_trained_8k\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true+NumExamples.8400/teacher_expl_dir\",\n",
    "#}\n",
    "\n",
    "#explainers_dirs = {\n",
    "#    \"attention\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers/TrainStudent/KLDCoeff.5+Seed.9/teacher_expl_dir\",\n",
    "#    \"attention_trained_lastlayers\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true/teacher_expl_dir\",\n",
    "#    \"attention_trained_alllayers\": \"/projects/tir2/users/pfernand/metaexpl-experiments/mlqe-xlmr-xlmr-alllayers-v2/TrainStudent/KLDCoeff.5+MetaExplicit.true+MetaLR.2e-4+MetaLearn.true/teacher_expl_dir\",\n",
    "#}\n",
    "\n",
    "seed=0\n",
    "max_len=256\n",
    "batch_size=1\n",
    "task=\"mlqe\"\n",
    "tokenizer=\"xlm-r\"\n",
    "model_dims = [12, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef6030",
   "metadata": {},
   "source": [
    "#### IMDB ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3aba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"/projects/tir2/users/pfernand/metaexpl-experiments/imdb-electra-electra-scalarmix/TrainTeacher/Baseline.baseline/teacher_dir\"\n",
    "explainers_dirs = {\n",
    "    \"attention\": \"/projects/tir2/users/pfernand/metaexpl-experiments/imdb-electra-electra-scalarmix/TrainStudent/KLDCoeff.5+NumExamples.2000/teacher_expl_dir\",\n",
    "    \"attention_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/imdb-electra-electra-scalarmix/TrainStudent/KLDCoeff.5+MetaLR.5e-4+MetaLearn.true+NumExamples.2000+Seed.9/teacher_expl_dir\",\n",
    "    \"gradient_input_explainer\": None,\n",
    "    \"integrated_gradients_explainer\": None\n",
    "}\n",
    "\n",
    "\n",
    "seed=0\n",
    "max_len=256\n",
    "batch_size=1\n",
    "task=\"imdb\"\n",
    "tokenizer=\"electra\"\n",
    "model_dims = [12, 4]\n",
    "modality=\"text\"\n",
    "\n",
    "from smat.utils import accuracy\n",
    "metric=accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e756f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"/projects/tir2/users/pfernand/metaexpl-experiments/imdb-electra-electra-scalarmix/TrainTeacher/Baseline.baseline/teacher_dir\"\n",
    "explainers_dirs = {\n",
    "    \"attention\": \"/projects/tir2/users/pfernand/metaexpl-experiments/imdb-electra-electra-scalarmix/TrainStudent/KLDCoeff.5+NumExamples.2000/teacher_expl_dir\",\n",
    "    \"attention_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/imdb-electra-electra-scalarmix/TrainStudent/KLDCoeff.5+MetaLR.5e-4+MetaLearn.true+NumExamples.2000+Seed.9/teacher_expl_dir\",\n",
    "    \"gradient_input_explainer\": None,\n",
    "    \"integrated_gradients_explainer\": None\n",
    "}\n",
    "\n",
    "\n",
    "seed=0\n",
    "max_len=256\n",
    "batch_size=1\n",
    "task=\"movie_rationales\"\n",
    "tokenizer=\"electra\"\n",
    "model_dims = [12, 4]\n",
    "modality=\"text\"\n",
    "from smat.utils import accuracy\n",
    "metric=accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe341746",
   "metadata": {},
   "source": [
    "#### CIFAR100 ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"/projects/tir2/users/pfernand/metaexpl-experiments/cifar100-vit-vit/TrainTeacher/Baseline.baseline/teacher_dir\"\n",
    "explainers_dirs = {\n",
    "    \"attention\": \"/projects/tir2/users/pfernand/metaexpl-experiments/cifar100-vit-vit/TrainStudent/KLDCoeff.5+Seed.9/teacher_expl_dir\",\n",
    "    \"attention_trained\": \"/projects/tir2/users/pfernand/metaexpl-experiments/cifar100-vit-vit/TrainStudent/KLDCoeff.5+MetaLR.5e-4+MetaLearn.true+Seed.9/teacher_expl_dir\",\n",
    "    \"gradient_input_explainer\": None\n",
    "}\n",
    "\n",
    "seed=0\n",
    "max_len=256\n",
    "batch_size=1\n",
    "task=\"cifar100\"\n",
    "tokenizer=\"vit-base\"\n",
    "model_dims = [12, 12]\n",
    "modality=\"image\"\n",
    "from smat.utils import accuracy\n",
    "metric=accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from smat.models import load_model\n",
    "from smat.explainers import *\n",
    "from smat.utils import PRNGSequence\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from transformers import ElectraTokenizerFast, BertTokenizerFast, XLMRobertaTokenizerFast, ViTFeatureExtractor\n",
    "\n",
    "if task == \"imdb\":\n",
    "    from smat.data.imdb import dataloader, load_data\n",
    "elif task == \"movie_rationales\":\n",
    "    from smat.data.movie_rationales import dataloader, load_data\n",
    "elif task == \"mlqe\":\n",
    "    from smat.data.mlqe import dataloader, load_data\n",
    "elif task == \"sst2\":\n",
    "    from smat.data.sst2 import dataloader, load_data\n",
    "elif task == \"cifar100\":\n",
    "    from smat.data.cifar100 import dataloader, load_data\n",
    "else:\n",
    "    raise ValueError(f\"Unknown task {task}\")\n",
    "    \n",
    "if tokenizer == \"electra\":\n",
    "    tokenizer = ElectraTokenizerFast.from_pretrained(\n",
    "        \"google/electra-small-discriminator\"\n",
    "    )\n",
    "elif tokenizer == \"mbert\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "elif tokenizer == \"xlm-r\":\n",
    "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "elif tokenizer == \"xlm-r-large\":\n",
    "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")\n",
    "elif tokenizer == \"vit-base\":\n",
    "    tokenizer = ViTFeatureExtractor.from_pretrained(\n",
    "            \"google/vit-base-patch16-224-in21k\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.nn as nn\n",
    "from entmax_jax.activations import sparsemax, entmax15\n",
    "\n",
    "jnp.set_printoptions(suppress=True)\n",
    "\n",
    "rngseq = PRNGSequence(seed)\n",
    "\n",
    "# create dummy inputs for model instantiation\n",
    "if modality == \"text\":\n",
    "    input_ids = jnp.ones((1, max_len), jnp.int32)\n",
    "    dummy_inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": jnp.ones_like(input_ids),\n",
    "            \"token_type_ids\": jnp.arange(jnp.atleast_2d(input_ids).shape[-1]),\n",
    "            \"position_ids\": jnp.ones_like(input_ids),\n",
    "    }\n",
    "elif modality == \"image\":\n",
    "    dummy_inputs = {\"pixel_values\": jnp.ones((1, 3, 224, 224))}\n",
    "\n",
    "#from ipdb import launch_ipdb_on_exception\n",
    "#with launch_ipdb_on_exception():\n",
    "model, params, dummy_state = load_model(model_dir, dummy_inputs, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.argmax(params[\"params\"][\"scalarmix\"][\"coeffs\"])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainers = {}\n",
    "\n",
    "for explainer_name, explainer_dir in explainers_dirs.items():\n",
    "    if explainer_dir is None:\n",
    "        import ipdb\n",
    "        with ipdb.launch_ipdb_on_exception():\n",
    "            explainers[explainer_name] = create_explainer(\n",
    "            next(rngseq), \n",
    "            dummy_inputs, \n",
    "            dummy_state,  \n",
    "            explainer_type=explainer_name,\n",
    "            model_extras={\"grad_fn\": model.apply(params, dummy_inputs, method=model.embeddings_grad_fn)},\n",
    "            )\n",
    "    else:\n",
    "        explainers[explainer_name] = load_explainer(\n",
    "            explainer_dir, \n",
    "            dummy_inputs, \n",
    "            dummy_state, \n",
    "            {\"grad_fn\": model.apply(params, dummy_inputs, method=model.embeddings_grad_fn)}\n",
    "        )\n",
    "       \n",
    "    if \"attention\" in explainer_name:\n",
    "        if explainers[explainer_name][0].normalize_head_coeffs:\n",
    "            if explainers[explainer_name][0].normalize_head_coeffs == \"sparsemax\":\n",
    "                normalizer_fn = sparsemax\n",
    "            elif explainers[explainer_name][0].normalize_head_coeffs == \"entmax\":\n",
    "                normalizer_fn = entmax15\n",
    "            else:\n",
    "                normalizer_fn = nn.softmax\n",
    "        else:\n",
    "            normalizer_fn = lambda x: x\n",
    "        \n",
    "        head_weights = normalizer_fn(explainers[explainer_name][1][\"params\"]['head_coeffs'])\n",
    "        print(len(head_weights))\n",
    "        for layer_idx in range(model_dims[0]):\n",
    "            print(f\"layer_{layer_idx:02d}\\t\", \"\\t\".join(f\"{w:.4f}\" for w in head_weights[layer_idx * model_dims[1]:(layer_idx+1) * model_dims[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b813e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"student\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "num_heads = 12 * 12\n",
    "def create_single_head_explainer(base_explainer, num_heads, head_no):\n",
    "    modified = flax.core.unfreeze(base_explainer[1])\n",
    "    head_coefs = [float('-inf') for _ in range(num_heads)]\n",
    "    head_coefs[head_no] = 1\n",
    "    modified[\"params\"][\"head_coeffs\"] = jnp.array(head_coefs)\n",
    "    explainer = deepcopy(base_explainer[0])\n",
    "    explainer.layer_idx = base_explainer[0].layer_idx\n",
    "    return (explainer, flax.core.freeze(modified))\n",
    "\n",
    "heads_p = sparsemax(explainers[\"attention_sparsemax_trained\"][1][\"params\"]['head_coeffs'])\n",
    "for i in range(num_heads):\n",
    "    if heads_p[i] > 0:\n",
    "        explainers[f\"head_{i}\"] = create_single_head_explainer(explainers[\"attention\"], num_heads, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def print_coloured(subwords, weights, max_alpha=0.8):\n",
    "    text = []\n",
    "    colour = [\"250, 133, 205\" if w <= 0 else \"135,206,250\" for w in weights]\n",
    "    weights = weights / jnp.max(weights)\n",
    "    for i, subword in enumerate(subwords):\n",
    "        if subword == \"<s>\":\n",
    "            subword = \"[CLS]\"\n",
    "        if subword == \"</s>\":\n",
    "            subword = \"[EOS]\"\n",
    "        if subword in (\"[CLS]\", \"[EOS]\", \"[SEP]\"):\n",
    "            continue\n",
    "        if subword[0] == \"‚ñÅ\":\n",
    "            subword = subword[1:]\n",
    "            if i:\n",
    "                text.append(\" \")\n",
    "        colorstr = f\"<span style='background-color:rgba({colour[i]},{weights[i] / max_alpha:.03f}'>{subword}</span>\"\n",
    "        text.append(colorstr)\n",
    "    display(Markdown(\" \".join(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def get_out_state(model, params, x):\n",
    "    \"\"\" Evaluation step \"\"\"\n",
    "    return model.apply(params, **x)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def get_explanation(explainer, explainer_params, params, x, state):\n",
    "    \"\"\" Evaluation step \"\"\"\n",
    "    return explainer.apply(\n",
    "        explainer_params, x, state, grad_fn=model.apply(params, x, method=model.embeddings_grad_fn)\n",
    "    )\n",
    "\n",
    "outs, labels = [], []\n",
    "num_samples = 100000\n",
    "for sample in data[:num_samples]:\n",
    "    if task == \"mlqe\":\n",
    "        label, inputs = sample[\"z_mean\"], f\"{sample['original']} </s> {sample['translation']}\"\n",
    "    if task == \"imdb\":\n",
    "        label, inputs = sample['label'], sample['text']\n",
    "    if task == \"movie_rationales\":\n",
    "        label, inputs = sample['label'], sample['review']\n",
    "    # extract token ids and subwords from tokenizer\n",
    "    x = dict(tokenizer([inputs], truncation=True, padding='max_length', max_length=max_len, return_tensors=\"jax\"))\n",
    "    num_tokens = jnp.sum(jnp.where(x['input_ids'] != 0, 1, 0))\n",
    "    bp_tokens = tokenizer.convert_ids_to_tokens(x['input_ids'][0][:num_tokens], skip_special_tokens=False)\n",
    "    # extract model state \n",
    "    out, state = get_out_state(model, params, x)\n",
    "    #print(f\"Label: {label}\")\n",
    "    #print(f\"Out: {out}\\n\")\n",
    "    labels.append(label)\n",
    "    outs.append(out[0][0])\n",
    "    for explainer, (expl_cls, expl_params) in []: # explainers.items():\n",
    "        print(f\"Explainer: {explainer}\")\n",
    "        explanation, logits = get_explanation(expl_cls, expl_params, params, x, state)\n",
    "        explanation = explanation[0][:num_tokens]\n",
    "        logits = logits[\"z\"][0][:num_tokens]\n",
    "        #print(expl_params)\n",
    "        if explainer == \"attention_query_col\":\n",
    "            col_explanation = explanation\n",
    "        if explainer == \"attention_query_col_trained\":\n",
    "            col_trained_explanation = explanation\n",
    "        #print(logits)\n",
    "        assert jnp.isclose(jnp.sum(explanation), 1, atol=1e-2), jnp.sum(explanation)\n",
    "        #print_coloured(bp_tokens, explanation)\n",
    "    #print(col_trained_explanation - col_explanation)\n",
    "    #print(f\"Attention Diff\")\n",
    "    #print_coloured(bp_tokens, col_trained_explanation - col_explanation)\n",
    "    #print(\"----------\")\n",
    "print(metric(jnp.array(outs), jnp.array(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entmax_jax.losses import softmax_loss\n",
    "\n",
    "logits = jnp.array[[1, 3, 2, -1, 10]]\n",
    "\n",
    "def topk_softmax(logits, topk=0.1, axis=-1):\n",
    "    threshold = -1e-7\n",
    "    numtoks = jnp.sum(jnp.where(logits > threshold, 1, 0), axis=axis)\n",
    "    topk_numtoks = jnp.maximum(jnp.round(numtoks * topk), 1)\n",
    "    unnorm_probs = jnp.where(jnp.argsort(logits, axis=axis) < topk_numtoks[:, None], 1, 0)\n",
    "    \n",
    "    return unnorm_probs / jnp.sum(unnorm_probs, axis=axis, keepdims=True)\n",
    "    \n",
    "\n",
    "expl_cls, expl_params = explainers[\"attention_explainer\"]\n",
    "def loss(state):\n",
    "    expl = get_explanation(expl_cls, expl_params, x, state)\n",
    "    return softmax_loss(expl, gold, mask=x[\"attention_mask\"])\n",
    "\n",
    "print(loss(state))\n",
    "print(jax.grad(loss)(state)[\"attentions\"][-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f53cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab476c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entmax_jax.losses import softmax_loss\n",
    "\n",
    "logits = jnp.array([[1., 3., 2., -1., 10.], [-1, 10, 2, 3, 4]])\n",
    "\n",
    "def batched_scatter(input, index, src):\n",
    "    \"\"\" 2-d batched scatter \"\"\"\n",
    "    idx = jnp.arange(input.shape[0]).reshape((-1, 1))\n",
    "    idx = jnp.repeat(idx, input.shape[1], 1)\n",
    "    return input.at[idx, index].set(src)\n",
    "\n",
    "def topk_softmax(logits, topk=0.1, temperature=1., axis=-1):\n",
    "    #TODO: currently only works for 2-d tensors with axis=-1\n",
    "    threshold = -1e7\n",
    "    numtoks = jnp.sum(jnp.where(logits > threshold, 1, 0), axis=axis)\n",
    "    topk_numtoks = jnp.maximum(jnp.round(numtoks * topk), 1)\n",
    "    \n",
    "    #obtain ranks from argsort\n",
    "    sorted_idx = jnp.argsort(logits, axis=axis)[:,::-1]\n",
    "    zeros = jnp.zeros(logits.shape, sorted_idx.dtype)\n",
    "    ranges = jnp.repeat(jnp.arange(logits.shape[-1]).reshape((1,-1)), logits.shape[0], 0)\n",
    "    ranks = batched_scatter(zeros, sorted_idx, ranges)\n",
    "    \n",
    "    topk_logits = jnp.where(ranks < topk_numtoks[:, None], logits * temperature, threshold)\n",
    "    return nn.softmax(topk_logits, axis=axis)\n",
    "\n",
    "def loss(logits):\n",
    "    return jnp.sum(topk_softmax(logits, topk=0.5)**2, axis=1).mean()\n",
    "\n",
    "print(topk_softmax(logits, topk=0.5, temperature=0.5))\n",
    "jax.grad(loss)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99357b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, FlaxViTForImageClassification\n",
    "from PIL import Image\n",
    "import jax\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = FlaxViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "params = model.params\n",
    "\n",
    "inputs = feature_extractor(images=image, return_tensors=\"np\")\n",
    "outputs = model.module.apply(params, **inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = jax.numpy.argmax(logits, axis=-1)\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7957bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"pixel_values\"].shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c7f28e66474da19e85c60f662bba9cf73619e12646aa700cede8ba845851a22"
  },
  "kernelspec": {
   "display_name": "Python [conda env:meta-expl] *",
   "language": "python",
   "name": "conda-env-meta-expl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
